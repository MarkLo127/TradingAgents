# -*- coding: utf-8 -*-
"""
LLM Output Post-Processing Filter
Fixes common LLM output errors including character corruption and format issues
"""
import re


def count_chinese_words(text: str) -> int:
    """
    Count Chinese characters in text (excluding markdown and tables)
    
    Args:
        text: Input text
        
    Returns:
        Number of Chinese characters
    """
    # Remove code blocks
    clean_text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
    # Remove tables
    clean_text = re.sub(r'\|.*?\|', '', clean_text, flags=re.MULTILINE)
    # Remove markdown formatting
    clean_text = re.sub(r'[#\*_`~\[\]\(\)]', '', clean_text)
    
    # Count Chinese characters (CJK Unified Ideographs)
    return len([c for c in clean_text if '\u4e00' <= c <= '\u9fff'])


def fix_common_llm_errors(text: str) -> str:
    """
    Fix common LLM character selection errors
    
    Args:
        text: LLM output text
        
    Returns:
        Corrected text
    """
    # Common character misuse patterns
    replacements = {
        # 'ç…‰' misuse - should be 'ç·´' (practice/train) in most contexts
        'ç…‰ç¿’': 'ç·´ç¿’',
        'è¨“ç…‰': 'è¨“ç·´',
        '**ç…‰**': '**ç·´**',
        'ï¼ˆç…‰': 'ï¼ˆç·´',
        'ç…‰ï¼‰': 'ç·´ï¼‰',
        
        # Other common errors (add as discovered)
        'çµ“é©—': 'ç¶“é©—',  # We saw this corruption before
    }
    
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    
    return text


def validate_and_warn(content: str, agent_name: str) -> list:
    """
    Validate report content and return list of warnings
    
    Args:
        content: Report content
        agent_name: Name of the agent
        
    Returns:
        List of warning messages
    """
    warnings = []
    
    # Check for suspicious 'ç…‰' character
    # Only flag if not in proper contexts like "å†¶ç…‰", "æç…‰", "é›ç…‰"
    if 'ç…‰' in content:
        proper_contexts = ['å†¶ç…‰', 'æç…‰', 'é›ç…‰', 'ç²¾ç…‰', 'ä¿®ç…‰']
        is_proper = any(ctx in content for ctx in proper_contexts)
        if not is_proper:
            # Find context around 'ç…‰'
            idx = content.find('ç…‰')
            context = content[max(0, idx-15):min(len(content), idx+15)]
            warnings.append(f"Suspicious 'ç…‰' character found. Context: ...{context}...")
    
    # Check word count
    word_count = count_chinese_words(content)
    if word_count < 800:
        warnings.append(f"Too short: {word_count} words (expected 800-1500)")
    elif word_count > 1500:
        warnings.append(f"Too long: {word_count} words (expected 800-1500)")
    
    # Check for truncation markers that shouldn't be there
    truncation_markers = ['...(å·²æˆªæ–·)', '...(å…§å®¹å·²æˆªæ–·)', '...(ç‚ºæ§åˆ¶é•·åº¦å·²ç²¾ç°¡)']
    for marker in truncation_markers:
        if marker in content:
            warnings.append(f"Found truncation marker: '{marker}'")
    
    if warnings:
        print(f"\nâš ï¸  {agent_name} Report Warnings:")
        for warning in warnings:
            print(f"   - {warning}")
    
    return warnings


def post_process_agent_output(content: str, agent_name: str, retry_callback=None) -> str:
    """
    Complete post-processing pipeline for agent output
    
    Args:
        content: Raw agent output
        agent_name: Name of the agent
        retry_callback: Optional function to call if validation fails
        
    Returns:
        Processed and validated content
    """
    # Step 1: Fix common errors
    content = fix_common_llm_errors(content)
    
    # Step 2: Validate and warn
    warnings = validate_and_warn(content, agent_name)
    
    # Step 3: Critical validation - retry if needed
    word_count = count_chinese_words(content)
    if (word_count < 800 or word_count > 1500) and retry_callback:
        print(f"\nğŸ”„ {agent_name}: Word count {word_count} out of range, triggering retry...")
        # Callback should regenerate the content
        # This is optional and should be implemented in the calling code
    
    return content
